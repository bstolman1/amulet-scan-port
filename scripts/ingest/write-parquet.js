/**
 * Parquet Writer Module - STREAMING VERSION
 * 
 * Handles writing ledger data to partitioned parquet files.
 * 
 * Optimizations:
 * - TRUE STREAMING writes (never holds entire file in memory)
 * - Timestamp-based filenames (no directory scans)
 * - Parallel concurrent writes (configurable)
 * - Buffer swap instead of copy
 * - Large I/O buffers (256KB)
 * - Row size limits to prevent memory explosion
 * - Retry with backoff (limited retries)
 */

import { createWriteStream, mkdirSync, existsSync, rmSync, readdirSync } from 'fs';
import { pipeline } from 'stream/promises';
import { Readable, Transform } from 'stream';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import { randomBytes } from 'crypto';
import { getPartitionPath } from './parquet-schema.js';
import { init, compress } from '@bokuweb/zstd-wasm';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Configuration - AGGRESSIVE DEFAULTS for speed
const DATA_DIR = process.env.DATA_DIR || join(__dirname, '../../data/raw');
const MAX_ROWS_PER_FILE = parseInt(process.env.MAX_ROWS_PER_FILE) || 50000;
const MAX_CONCURRENT_WRITES = parseInt(process.env.MAX_CONCURRENT_WRITES) || 20;
const IO_BUFFER_SIZE = 1024 * 1024; // 1MB buffer
const ZSTD_LEVEL = parseInt(process.env.ZSTD_LEVEL) || 3; // 1=fastest, 3=balanced

// Initialize ZSTD once at module load
let zstdReady = false;
async function ensureZstdReady() {
  if (!zstdReady) {
    await init();
    zstdReady = true;
  }
}
const MAX_ROW_SIZE_BYTES = 10 * 1024 * 1024; // 10MB max per row - skip larger ones
const MAX_RETRIES = 2; // Fewer retries - fail fast

// In-memory buffers for batching
let updatesBuffer = [];
let eventsBuffer = [];
let currentMigrationId = null;

// Async write queue with parallel execution
let writeQueue = [];
let activeWrites = 0;

/**
 * Ensure directory exists
 */
function ensureDir(dirPath) {
  if (!existsSync(dirPath)) {
    mkdirSync(dirPath, { recursive: true });
  }
}

/**
 * Generate unique filename using timestamp + random suffix (no directory scan needed)
 */
function generateFileName(prefix) {
  const ts = Date.now();
  const rand = randomBytes(4).toString('hex');
  return `${prefix}-${ts}-${rand}.jsonl.zst`;
}

/**
 * Create a generator that yields each row as a JSON line
 * This avoids building the entire content string in memory
 */
function* rowGenerator(rows) {
  for (const row of rows) {
    yield JSON.stringify(row) + '\n';
  }
}

/**
 * Write rows to JSON-lines file using ZSTD compression
 * Uses Buffer array to avoid string length limits
 * ZSTD is ~2-3x faster than gzip with better compression
 */
async function writeJsonLines(rows, filePath) {
  await ensureZstdReady();
  
  // Build content as array of Buffers (avoids string length limit)
  const chunks = [];
  for (const row of rows) {
    chunks.push(Buffer.from(JSON.stringify(row) + '\n', 'utf8'));
  }
  
  // Concatenate all buffers
  const input = Buffer.concat(chunks);
  
  // Compress with ZSTD
  const compressed = compress(input, ZSTD_LEVEL);
  
  // Write compressed data
  const dest = createWriteStream(filePath, { highWaterMark: IO_BUFFER_SIZE });
  await new Promise((resolve, reject) => {
    dest.write(compressed, (err) => {
      if (err) reject(err);
      else dest.end(resolve);
    });
  });
}

/**
 * Process the async write queue with parallel execution
 */
async function processWriteQueue() {
  // Start multiple writes concurrently up to MAX_CONCURRENT_WRITES
  while (writeQueue.length > 0 && activeWrites < MAX_CONCURRENT_WRITES) {
    const task = writeQueue.shift();
    activeWrites++;
    
    // Execute write without awaiting (parallel)
    executeWrite(task).finally(() => {
      activeWrites--;
      // Trigger next batch when a write completes
      setImmediate(() => processWriteQueue());
    });
  }
}

/**
 * Execute a single write task with limited retries
 */
async function executeWrite(task) {
  try {
    await task.execute();
    console.log(`üìù Wrote ${task.count} ${task.type} to ${task.file}`);
  } catch (err) {
    task.retries = (task.retries || 0) + 1;
    
    if (task.retries >= MAX_RETRIES) {
      console.error(`‚ùå Write failed permanently for ${task.file} after ${MAX_RETRIES} retries:`, err.message);
      // Drop the task - don't requeue forever
      return;
    }
    
    console.error(`‚ö†Ô∏è Write error for ${task.file} (retry ${task.retries}/${MAX_RETRIES}):`, err.message);
    // Exponential backoff: 1s, 2s, 4s
    const delay = 1000 * Math.pow(2, task.retries - 1);
    await new Promise(r => setTimeout(r, delay));
    writeQueue.unshift(task);
  }
}

/**
 * Filter out oversized rows that could cause memory issues
 */
function filterOversizedRows(rows, type) {
  const filtered = [];
  let skipped = 0;
  
  for (const row of rows) {
    try {
      const size = Buffer.byteLength(JSON.stringify(row));
      if (size > MAX_ROW_SIZE_BYTES) {
        skipped++;
      } else {
        filtered.push(row);
      }
    } catch {
      // If we can't even stringify it, skip it
      skipped++;
    }
  }
  
  if (skipped > 0) {
    console.warn(`‚ö†Ô∏è Skipped ${skipped} oversized ${type} (>${MAX_ROW_SIZE_BYTES / 1024 / 1024}MB each)`);
  }
  
  return filtered;
}

/**
 * Queue a write task for async parallel processing
 */
function queueWrite(rows, filePath, type) {
  // Filter out problematic rows before queueing
  const safeRows = filterOversizedRows(rows, type);
  
  if (safeRows.length === 0) {
    console.log(`‚ö†Ô∏è No ${type} to write after filtering oversized rows`);
    return null;
  }
  
  const task = {
    type,
    file: filePath,
    count: safeRows.length,
    retries: 0,
    execute: () => writeJsonLines(safeRows, filePath)
  };
  
  writeQueue.push(task);
  
  // Start processing queue in background (non-blocking)
  setImmediate(() => processWriteQueue());
  
  return { file: filePath, count: safeRows.length, queued: true };
}

/**
 * Add updates to buffer
 */
export async function bufferUpdates(updates) {
  updatesBuffer.push(...updates);
  
  if (updatesBuffer.length >= MAX_ROWS_PER_FILE) {
    return await flushUpdates();
  }
  return null;
}

/**
 * Add events to buffer
 */
export async function bufferEvents(events) {
  eventsBuffer.push(...events);
  
  if (eventsBuffer.length >= MAX_ROWS_PER_FILE) {
    return await flushEvents();
  }
  return null;
}

/**
 * Flush updates buffer to file (async queued)
 */
export async function flushUpdates() {
  if (updatesBuffer.length === 0) return null;
  
  const timestamp = updatesBuffer[0]?.timestamp || new Date();
  const migrationId = currentMigrationId || updatesBuffer[0]?.migration_id || null;
  const partition = getPartitionPath(timestamp, migrationId);
  const partitionDir = join(DATA_DIR, partition);
  
  ensureDir(partitionDir);
  
  // Use timestamp-based filename (no directory scan)
  const fileName = generateFileName('updates');
  const filePath = join(partitionDir, fileName);
  
  // Swap buffer reference instead of copying (faster)
  const rowsToWrite = updatesBuffer;
  updatesBuffer = [];
  
  // Queue write for async parallel processing
  return queueWrite(rowsToWrite, filePath, 'updates');
}

/**
 * Flush events buffer to file (async queued)
 */
export async function flushEvents() {
  if (eventsBuffer.length === 0) return null;
  
  const timestamp = eventsBuffer[0]?.timestamp || new Date();
  const migrationId = currentMigrationId || eventsBuffer[0]?.migration_id || null;
  const partition = getPartitionPath(timestamp, migrationId);
  const partitionDir = join(DATA_DIR, partition);
  
  ensureDir(partitionDir);
  
  // Use timestamp-based filename (no directory scan)
  const fileName = generateFileName('events');
  const filePath = join(partitionDir, fileName);
  
  // Swap buffer reference instead of copying (faster)
  const rowsToWrite = eventsBuffer;
  eventsBuffer = [];
  
  // Queue write for async parallel processing
  return queueWrite(rowsToWrite, filePath, 'events');
}

/**
 * Flush all buffers and wait for write queue to complete
 */
export async function flushAll() {
  const results = [];
  
  // Flush any remaining buffered data
  const updatesResult = await flushUpdates();
  if (updatesResult) results.push(updatesResult);
  
  const eventsResult = await flushEvents();
  if (eventsResult) results.push(eventsResult);
  
  // Wait for all queued writes to complete
  while (writeQueue.length > 0 || activeWrites > 0) {
    await new Promise(r => setTimeout(r, 100));
  }
  
  return results;
}

/**
 * Get buffer and queue stats
 */
export function getBufferStats() {
  return {
    updates: updatesBuffer.length,
    events: eventsBuffer.length,
    maxRowsPerFile: MAX_ROWS_PER_FILE,
    queuedWrites: writeQueue.length,
    activeWrites,
    maxConcurrentWrites: MAX_CONCURRENT_WRITES,
  };
}

/**
 * Wait for write queue to drain (useful before shutdown)
 */
export async function waitForWrites() {
  while (writeQueue.length > 0 || activeWrites > 0) {
    await new Promise(r => setTimeout(r, 100));
  }
}

/**
 * Convert JSON-lines to parquet using DuckDB
 * Run this as a separate step after ingestion
 */
export function createConversionScript() {
  return `
-- DuckDB script to convert JSON-lines to Parquet
-- Run: duckdb -c ".read convert-to-parquet.sql"
-- Supports .jsonl.zst (ZSTD compressed) files

-- Convert updates with optimized settings
COPY (
  SELECT * FROM read_json_auto('data/raw/**/updates-*.jsonl.zst')
) TO 'data/raw/updates.parquet' (
  FORMAT PARQUET, 
  COMPRESSION ZSTD,
  ROW_GROUP_SIZE 100000
);

-- Convert events with optimized settings
COPY (
  SELECT * FROM read_json_auto('data/raw/**/events-*.jsonl.zst')
) TO 'data/raw/events.parquet' (
  FORMAT PARQUET, 
  COMPRESSION ZSTD,
  ROW_GROUP_SIZE 100000
);
`;
}

/**
 * Set current migration ID for partitioning
 */
export function setMigrationId(id) {
  currentMigrationId = id;
}

/**
 * Clear migration ID
 */
export function clearMigrationId() {
  currentMigrationId = null;
}

/**
 * Purge all data files for a specific migration
 * Call this after migration data has been processed/uploaded to free disk space
 */
export function purgeMigrationData(migrationId) {
  const migrationPrefix = `migration_id=${migrationId}`;
  let deletedFiles = 0;
  let deletedDirs = 0;
  
  if (!existsSync(DATA_DIR)) {
    console.log(`   ‚ÑπÔ∏è Data directory doesn't exist, nothing to purge`);
    return { deletedFiles: 0, deletedDirs: 0 };
  }
  
  // Find and delete migration-specific partition directories
  const entries = readdirSync(DATA_DIR, { withFileTypes: true });
  
  for (const entry of entries) {
    if (entry.isDirectory() && entry.name.startsWith(migrationPrefix)) {
      const dirPath = join(DATA_DIR, entry.name);
      try {
        rmSync(dirPath, { recursive: true, force: true });
        deletedDirs++;
        console.log(`   üóëÔ∏è Deleted partition: ${entry.name}`);
      } catch (err) {
        console.error(`   ‚ùå Failed to delete ${dirPath}: ${err.message}`);
      }
    }
  }
  
  console.log(`   ‚úÖ Purged migration ${migrationId}: ${deletedDirs} directories removed`);
  return { deletedFiles, deletedDirs };
}

/**
 * Purge all data in the raw directory
 */
export function purgeAllData() {
  if (!existsSync(DATA_DIR)) {
    console.log(`   ‚ÑπÔ∏è Data directory doesn't exist, nothing to purge`);
    return;
  }
  
  try {
    rmSync(DATA_DIR, { recursive: true, force: true });
    mkdirSync(DATA_DIR, { recursive: true });
    console.log(`   ‚úÖ Purged all data from ${DATA_DIR}`);
  } catch (err) {
    console.error(`   ‚ùå Failed to purge data: ${err.message}`);
  }
}

export default {
  bufferUpdates,
  bufferEvents,
  flushUpdates,
  flushEvents,
  flushAll,
  getBufferStats,
  waitForWrites,
  createConversionScript,
  setMigrationId,
  clearMigrationId,
  purgeMigrationData,
  purgeAllData,
};
