name: Backfill Ledger History

on:
  schedule:
    - cron: '0 0 * * 0'  # Run every Sunday at midnight (00:00 UTC)
  workflow_dispatch:
    inputs:
      worker_count:
        description: 'Number of parallel workers (1=single-threaded, 8-12=parallel)'
        required: false
        type: number
        default: 1
      migration_id:
        description: 'Specific migration ID to backfill (optional, leave empty for all)'
        required: false
        type: string

# Prevent concurrent runs that could conflict with database writes
concurrency:
  group: ledger-ingestion
  cancel-in-progress: false # Queue instead of canceling

jobs:
  setup:
    runs-on: self-hosted
    outputs:
      workers: ${{ steps.create-matrix.outputs.workers }}
      worker_count: ${{ steps.create-matrix.outputs.worker_count }}
    steps:
      - name: Create worker matrix
        id: create-matrix
        shell: bash
        run: |
          COUNT=${{ inputs.worker_count || 1 }}
          
          # Build JSON array without jq dependency
          WORKERS='['
          for i in $(seq 0 $(($COUNT - 1))); do
            if [ $i -gt 0 ]; then
              WORKERS="${WORKERS},"
            fi
            WORKERS="${WORKERS}{\"id\":$i}"
          done
          WORKERS="${WORKERS}]"
          
          echo "workers=$WORKERS" >> $GITHUB_OUTPUT
          echo "worker_count=$COUNT" >> $GITHUB_OUTPUT
          echo "Created $COUNT workers: $WORKERS"

  backfill:
    needs: setup
    runs-on: self-hosted
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJson(needs.setup.outputs.worker_count) }}
      matrix:
        worker: ${{ fromJson(needs.setup.outputs.workers) }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies
        run: |
          npm init -y
          npm install axios @supabase/supabase-js pg pg-copy-streams

      - name: Run backfill worker ${{ matrix.worker.id }}
        env:
          BASE_URL: https://scan.sv-1.global.canton.network.sync.global/api/scan
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
          SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
          BACKFILL_PAGE_SIZE: "200"
          WORKER_ID: ${{ matrix.worker.id }}
          WORKER_COUNT: ${{ needs.setup.outputs.worker_count }}
          MIGRATION_ID: ${{ inputs.migration_id || '' }}
        run: |
          if [ "${{ needs.setup.outputs.worker_count }}" -gt 1 ]; then
            echo "üöÄ Starting parallel backfill worker ${{ matrix.worker.id }} of ${{ needs.setup.outputs.worker_count }}"
          else
            echo "üöÄ Starting single-threaded backfill"
          fi
          node scripts/fetch-backfill-history.js
        timeout-minutes: 1440  # 24 hours max per worker

  verify:
    needs: backfill
    runs-on: self-hosted
    steps:
      - name: Report completion
        run: |
          echo "‚úÖ Backfill complete"
          if [ "${{ needs.setup.outputs.worker_count }}" -gt 1 ]; then
            echo "üìä All ${{ needs.setup.outputs.worker_count }} parallel workers finished"
            echo ""
            echo "‚ö†Ô∏è  IMPORTANT: Don't forget to run scripts/restore-after-backfill.sql"
            echo "   to restore indexes and make tables LOGGED again!"
          fi
